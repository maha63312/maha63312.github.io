---
layout: portfolio_post
title: "Uge 41 – Vurdering af brug af Large Language Model"
week: 41
elective:
  - "Backend-udvikling og API-design"
  - "Machine Learning og AI"
goals: |
  Undersøge om implementering af en LLM kan være med til at berige metadata yderligere.
process: |
  Udvide testdata med billeder, der ikke eksisterer på nettet, for at vurdere GVC’s evne til at berige JSON-skemaet med metadata.  
  Teste LLM ved brug af det data, der returneres fra GCV, for at se om den kan udlede mere metadata ved brug af en udførlig prompt.
result: |
  **Google Vision Cloud**  
  Jeg har tidligere gjort brug af testdata, som findes på nettet (se her: [Uge 39](https://maha63312.github.io/2025/09/26/uge39.html)). Det var derfor vigtigt at teste med data — altså billeder — der er originale. Det betyder, at vi har fået taget billeder af maskiner, der står udenfor, således at vi kan teste tjenesten uden at GVC kan finde billedet på nettet selv.  

  ![Minigraver](/assets/images/minigraver.jfif)

  Det første billede består af en minigraver, der har et dozerblad på forsiden. Jeg sender billedet til GVC ved brug af Postman og får returneret testdata, jeg kan bruge til at justere heuristikken i koden, der sorterer det returnerede data. Efter at have kørt smoketesten står jeg tilbage med dette resultat:  

  ![Hitachi](/assets/images/hitatchi.png)

  Som man kan se, er den ikke i stand til at genkende maskinen ud over, at der er tale om en model fra Hitachi. Den tror desuden, at det er en bulldozer ud fra *Web_best_guess*. Jeg kan se på testresultatet, der er returneret fra GVC, at den er i stand til at finde samme maskine på andre billeder, som er taget, men at disse billeder ikke indeholder labels eller beskrivelser, der kan hjælpe den med at identificere maskinen. Jeg forsøger derfor med et andet originalt billede.

  ![Hjullæsser](/assets/images/hjullæsser.jfif)

  Billedet sendes afsted ved brug af Postman, således at jeg har nyt testdata til min smoketest. Denne gang returnerer den dette resultat:  

  ![Hjul](/assets/images/hjull.png)

  GVC kan finde kategorier som “Wheel” og “Heavy Equipment”, som sorteres væk ved brug af sortering inde i koden.  Den står derfor tilbage med enten modelnummer og logo, eller kun en af delene, alt efter hvad der er inkluderet på billedet.  

  Det er tydeligt, at der skal stilles krav til brugeren af applikationen i form af, hvad de tager billeder af. For at systemet skal være så nøjagtigt som muligt, er det en fordel, at billedet inkluderer både modelnummer og logo på den aktuelle maskine. Det er også tydeligt, at hvis GVC ikke finder et billedematch med en tilhørende detaljeret modelbeskrivelse, vil den ikke returnere noget af værdi under Web-Detection. Det viser, at microservicen ikke kan basere sig på brug af GVC alene. 
  
  Som udgangspunkt ser det ud til, at Text-Detection og Logo-Detection er de features, som er mest nyttige i vores tilfælde, hvor Web-Detection kan være en feature, der måske/måske ikke returnerer værdifuld data. De andre features er gode til at levere brede, generiske data, som er udenfor scope i dette tilfælde da vi ønsker finkategorisering af maskiner.   

  Ved en hurtig manuel test af OpenAI, hvor jeg laver en prompt med modelnavn og angiver, at det er tale om byggemaskiner, kan LLM returnere både mærke og kategori. Den er altså i stand til at berige metadata yderligere. Det skal dog nævnes, at LLM kan hallucinere og returnere data, som ikke eksisterer.  Derfor skal prompt og JSON-format, som skal returneres, være omhyggeligt opbygget for at forhindre dette.  

  Denne test har givet en bedre forståelse af, hvordan LLM og GVC kan supplere hinanden i forhold til at forbedre kvaliteten af metadata i vores microservice.

next_plan: "Uge 42: Overveje hvordan LLM kan implementeres ved brug af JSON-skema og prompt. Modulere HLD og LLD ved den nye implementation."
resources:
  - { title: "Postman", url: "https://www.postman.com/" }
---
